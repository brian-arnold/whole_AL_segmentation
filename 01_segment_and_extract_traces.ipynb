{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pixel intensity thresholding to segment the entire mosquito antennal lobe (AL)\n",
    "\n",
    "This notebook is kept tidy by modularizing the data and code into separate python files that are imported here.\n",
    "\n",
    "See `experiment_info.py` to specify important details related to the experiment. Within this file, important information gets stored in the `params` dictionary.\n",
    "\n",
    "If you have different experiments, simply store the information in a separate python file and change the name `experiment_info` in the cell below to correspond to the file with the information from this other experiment.\n",
    "\n",
    "**NOTES**: \n",
    "- There should be as many .tif videos as there are odors, and it is assumed that these videos are alphanumerically labeled in the same order as they appear in `odor_string`. I.e., if we sort the names of the videos, the first one should correspond to the first odor in `odor_string`.\n",
    "- For computational speed, this code should be run wherever the raw data are stored. If you mount the file system where the data are stored (e.g. PNI cluster) and run the code on your local machine, it may go very slow as the data has to transfer over the network. I currently use this notebook for interactive work, but when I'm satisfied with the results for a few samples, I export this notebook as a python script using `jupyter nbconvert --to script 01_segment_and_extract_traces.ipynb` and then execute this script as a job using SLURM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make directory to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'results'\n",
    "import os\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_info import samples, data_dir, puffs, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in `experiment_info.py` along with `functions.py`, which has some custom functions used here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 13:34:49.694147: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-28 13:34:49.694249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-28 13:34:49.743577: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory: /jukebox/mcbride/bjarnold/new_analysis/data/Mar_22_2024/1_RegisteredBrains\n",
      "Number of samples: 1\n",
      "Number of odors: 72\n",
      "x,y,z dimensions: 128 128 24\n",
      "Number of frames to analyze: 112\n",
      "Number of initial frames for df/f normalization: 20\n"
     ]
    }
   ],
   "source": [
    "# import custom functions\n",
    "import functions as fn\n",
    "\n",
    "print(f'data directory: {data_dir}')\n",
    "print(f'Number of samples: {len(samples)}')\n",
    "num_odors = len(puffs)\n",
    "\n",
    "print(f'Number of odors: {num_odors}')\n",
    "print(f'x,y,z dimensions:', params['x_dim'], params['y_dim'], params['z_dim'])\n",
    "print(f'Number of frames to analyze:', params['n_frames_to_analyze'])\n",
    "print(f'Number of initial frames for df/f normalization:', params['background_frames'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import caiman as cm\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get full file paths for all videos, per sample.\n",
    "Note that this finds **all** videos per sample. We potentially select a subset of these later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = {}\n",
    "for samp in samples:\n",
    "    v = glob.glob(f\"{data_dir}/{samp}/*.registered.tif\")\n",
    "    v = sorted(v)\n",
    "    num_vids = len(v)\n",
    "    assert num_vids == len(puffs), f\"I found {num_vids} videos for sample {samp}, but there are {len(puffs)} odors. The number of videos and odors should be equivalent.\"\n",
    "    videos[samp] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each sample, concatenate all videos (one per odor) and threshold to segment AL.\n",
    "For thresholding, we are currently using `ski.filters.threshold_otsu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of volume containing AL segment for sample 230913_ORL_GCaMP6f_F1:  0.10248565673828125\n"
     ]
    }
   ],
   "source": [
    "# data reloaded to ensure cell runs independently\n",
    "binary_masks = {}\n",
    "for samp in videos:\n",
    "    vids = videos[samp]\n",
    "    Y_list = fn.load_videos_into_list(vids, params, normalize=False)\n",
    "    Y = cm.concatenate(Y_list)\n",
    "\n",
    "    binary_mask = fn.find_binary_mask(Y)\n",
    "    print(f\"fraction of volume containing AL segment for sample {samp}: \", np.sum(binary_mask)/(params['x_dim']*params['y_dim']*params['z_dim']))\n",
    "    binary_masks[samp] = binary_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save binary masks to use in downstream analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{results_dir}/binary_masks.pkl', 'wb') as f:\n",
    "    pickle.dump(binary_masks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at 2D projections of 3D binary masks\n",
    "Check out all the .png files that get created in the `binary_mask_plots` subdirectory in results_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{results_dir}/binary_mask_plots', exist_ok=True)\n",
    "colors = [(0, 0, 0, 0), (0, 0, 0, 0.1)]  # RGBA tuples, the 1st color's alpha set to 0 to make transparent so white values are ignores, black values are partially transparent; 0's mapped to 1st color, 1's mapped to 2nd color\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "for i,samp in enumerate(samples):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "    for i in range(0,params['z_dim']):\n",
    "        plt.imshow(binary_masks[samp][:,:,i], cmap=cmap)\n",
    "    plt.title(samples[0])\n",
    "    sns.despine()\n",
    "    plt.savefig(f'{results_dir}/binary_mask_plots/{samp}_binary_mask.png', dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within each AL segment, compute mean activity over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished sample 1\n"
     ]
    }
   ],
   "source": [
    "mean_activity_within_segment = {}\n",
    "for i,samp in enumerate(videos):\n",
    "    vids = videos[samp]\n",
    "    Y_list = fn.load_videos_into_list(vids, params, normalize=True) # note normalize = True!\n",
    "    Y = cm.concatenate(Y_list)\n",
    "    mean_activity_within_segment[samp] = fn.extract_mean_activity_within_binary_mask(Y, binary_masks[samp], params)\n",
    "    print(f'finished sample {i+1}')\n",
    "    \n",
    "mean_activity_df = pd.DataFrame.from_dict(mean_activity_within_segment)\n",
    "mean_activity_df.to_csv(f'{results_dir}/mean_activity_within_segment.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract the paraffin signal from each trace (nonfunctional after code change; not updated because results never used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_activity_within_segment_paraffin_subtracted = fn.subtract_paraffin_trace(mean_activity_within_segment, odor_of_interest_indices, odor_list, odor_encodings, params['n_frames_to_analyze'])\n",
    "# mean_activity_paraffin_subtracted_df = pd.DataFrame.from_dict(mean_activity_within_segment_paraffin_subtracted)\n",
    "# mean_activity_paraffin_subtracted_df.to_csv(f'{results_dir}/mean_activity_within_segment_paraffin_subtracted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot activity traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(16, 4))\n",
    "\n",
    "for i,samp in enumerate(mean_activity_within_segment):    \n",
    "    plt.plot(mean_activity_within_segment[samp] + i*0.02, c='black')  # Offset each trace by i*3\n",
    "    # print sample name on the right\n",
    "    plt.text(len(mean_activity_within_segment[samp])*1.02, i*0.02, samp, color='black')\n",
    "\n",
    "# print the names of the odors on the x-axis\n",
    "odor_names = []\n",
    "positions = []\n",
    "for i,puff in enumerate(puffs):\n",
    "    odor_names.append(puff.odor_name)\n",
    "    positions.append(i*params['n_frames_to_analyze'] + params['n_frames_to_analyze']/2)\n",
    "plt.xticks(positions, odor_names, rotation=90)\n",
    "\n",
    "# draw vertical lines to separate odors\n",
    "for i in range(len(puffs)):\n",
    "    plt.axvline((i+1)*params['n_frames_to_analyze'], color=\"black\", linestyle=\"--\", alpha=0.1)\n",
    "\n",
    "plt.yticks([])\n",
    "# supress grid lines\n",
    "plt.grid(False)\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig(f'{results_dir}/signal_traces.png', dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each odor, using the mean activity traces, get the maximum intensity during the frames corresponding to that odor.\n",
    "\n",
    "While were at it, let's also get the exact frame in which the segment was at it's max activity and store in the `argmaxs_by_samp` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload the functions module to make sure we are using the latest version\n",
    "import importlib\n",
    "importlib.reload(fn)\n",
    "\n",
    "maxs_by_samp, argmaxs_by_samp = fn.compute_max_responses(mean_activity_within_segment, puffs, params['n_frames_to_analyze'])\n",
    "aucs_by_samp = fn.calculate_AUC(mean_activity_within_segment, puffs, params, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the mean activity in each segment, subtract out the activity observed from the paraffin odor (negative control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_activity_within_segment_paraffin_subtracted = fn.subtract_paraffin_trace(mean_activity_within_segment, odor_of_interest_indices, odor_list, odor_encodings, params['n_frames_to_analyze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxs_by_samp_paraffin_subtracted, _ = fn.compute_max_responses(mean_activity_within_segment_paraffin_subtracted, odor_of_interest_indices, odor_list, odor_encodings, params['n_frames_to_analyze'])\n",
    "# aucs_by_samp_paraffin_subtracted = fn.calculate_AUC(mean_activity_within_segment_paraffin_subtracted, odor_of_interest_indices, odor_list, odor_encodings, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the data to a DataFrame and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "peak_max_df = fn.convert_to_df(maxs_by_samp, puffs)\n",
    "peak_auc_df = fn.convert_to_df(aucs_by_samp, puffs)\n",
    "peak_max_df.to_csv(f'{results_dir}/peak_max_df.csv', index=False)\n",
    "peak_auc_df.to_csv(f'{results_dir}/peak_auc_df.csv', index=False)\n",
    "        \n",
    "# peak_max_paraffin_df = fn.convert_to_df(maxs_by_samp_paraffin_subtracted, odor_order)\n",
    "# peak_auc_paraffin_df = fn.convert_to_df(aucs_by_samp_paraffin_subtracted, odor_order)\n",
    "# peak_max_paraffin_df.to_csv(f'{results_dir}/peak_max_paraffin_df.csv', index=False)\n",
    "# peak_auc_paraffin_df.to_csv(f'{results_dir}/peak_auc_paraffin_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each odor, get the frame corresponding to the maximum activity\n",
    "\n",
    "-within compute_max_responses, get index of frame of max activity\n",
    "\n",
    "-store in dict with odor_name as key, index as value\n",
    "\n",
    "-go through each odor_of_interest_indices, get name of odor and load the video for that index\n",
    "\n",
    "-extract frame of corresponding to index of max intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, puff in enumerate(puffs):\n",
    "    # i=11 is nonanal4.56\n",
    "    # i=16 is decanal5.53\n",
    "    for samp in samples:\n",
    "        os.makedirs(f'{results_dir}/images_at_max_intensity_per_odor/{samp}', exist_ok=True)\n",
    "        # only look at the first trial; odors with indices > 36 are repeats from second trial.\n",
    "        if puff.trial == 1:\n",
    "            odor_name = puff.odor_name\n",
    "            frame_at_max = argmaxs_by_samp[samp][odor_name][0]\n",
    "            Y = cm.load(videos[samp][puff.number])\n",
    "            Y = fn.reshape(Y, params['x_dim'], params['y_dim'], params['z_dim'])\n",
    "            Y = fn.background_normalize(Y, params['background_frames'])\n",
    "            Y = Y[frame_at_max]\n",
    "            file_name = f'{results_dir}/images_at_max_intensity_per_odor/{samp}/{odor_name}.pkl'\n",
    "            with open(file_name, 'wb') as f:\n",
    "                pickle.dump(Y, f)\n",
    "            # projection = np.mean(Y, axis=2)\n",
    "            # fig, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "            # plt.imshow(projection, cmap='bone')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caiman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
